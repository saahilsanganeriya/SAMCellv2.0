{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import monai\n",
    "from tqdm import tqdm\n",
    "import wandb\n",
    "\n",
    "from transformers import SamProcessor, SamImageProcessor\n",
    "import torch\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch import nn\n",
    "\n",
    "from model import FinetunedSAM\n",
    "from utils import lr_warmup, init_wandb, log_wandb\n",
    "from SAMDataset import SAMDataset\n",
    "\n",
    "dataset_path = '../dataset_processing/CellPose-train'\n",
    "img_path = dataset_path + 'imgs.npy'\n",
    "flow_path = dataset_path + 'flows.npy'\n",
    "\n",
    "sam_model = 'facebook/sam-vit-base'\n",
    "output_path = '../checkpoints/samcell2.0-flows'\n",
    "num_epochs = 40\n",
    "do_log_wandb = False\n",
    "\n",
    "#setup custom dataset\n",
    "print('loading dataset...')\n",
    "processor = SamProcessor.from_pretrained(sam_model)\n",
    "dataset = SAMDataset(img_path, flow_path, processor, weight_path=None)\n",
    "train_dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "print('loaded {} images'.format(len(dataset)))\n",
    "\n",
    "print('loading model...')\n",
    "modelHelper = FinetunedSAM(sam_model, finetune_vision=False, finetune_prompt=False, finetune_decoder=True)\n",
    "model = modelHelper.get_model()\n",
    "\n",
    "print('training...')\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model.to(device)\n",
    "\n",
    "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=0.1, betas=(0.9, 0.999))\n",
    "scheduler = LambdaLR(optimizer, lr_lambda=lr_warmup)\n",
    "sigmoid = nn.Sigmoid()\n",
    "l2_loss = nn.MSELoss(reduction='mean')\n",
    "\n",
    "if do_log_wandb:\n",
    "    run = init_wandb()\n",
    "   \n",
    "model.train()\n",
    "step = 0\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_losses = []\n",
    "    for batch in tqdm(train_dataloader):\n",
    "      # forward pass\n",
    "      outputs = model(pixel_values=batch[\"pixel_values\"].to(device),\n",
    "                      multimask_output=True)\n",
    "\n",
    "      # compute loss\n",
    "      predicted_masks = outputs.pred_masks.squeeze(1)\n",
    "      ground_truth_masks = batch[\"ground_truth_mask\"].float().to(device)\n",
    "      step_loss = l2_loss(sigmoid(predicted_masks[:, 0]), ground_truth_masks)\n",
    "\n",
    "      # backward pass (compute gradients of parameters w.r.t. loss)\n",
    "      optimizer.zero_grad()\n",
    "      step_loss.backward()\n",
    "\n",
    "      # optimize\n",
    "      optimizer.step()\n",
    "      scheduler.step()\n",
    "      epoch_losses.append(step_loss.item())\n",
    "\n",
    "      #log (if needed)\n",
    "      if do_log_wandb:\n",
    "        log_wandb(run, step, float(scheduler.get_last_lr()[0]), step_loss.item())\n",
    "\n",
    "      step += 1\n",
    "\n",
    "    print(f'EPOCH: {epoch}')\n",
    "    print(f'Mean loss: {np.mean(epoch_losses)}')\n",
    "\n",
    "print('done training. saving model to {}...'.format(output_path))\n",
    "model.save_pretrained(output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
